#!/usr/bin/env python3
"""
LLM Inline - OpenAI-compatible command line LLM interface

Usage: llmi ask "your question here" [--file file_path]
"""

import os
import sys
import json
import subprocess
import argparse
from openai import OpenAI
from pathlib import Path


def read_file_content(file_path: str) -> dict:
    """
    è¯»å–æ–‡ä»¶å†…å®¹ï¼Œè¿”å›æ–‡ä»¶ä¿¡æ¯å­—å…¸
    æ”¯æŒç›¸å¯¹è·¯å¾„è½¬æ¢
    """
    try:
        # æ”¯æŒç›¸å¯¹è·¯å¾„
        abs_path = Path(file_path).expanduser().resolve()
        
        if not abs_path.exists():
            return {"error": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"}
        
        if not abs_path.is_file():
            return {"error": f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶: {file_path}"}
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œé¿å…ä¸Šä¼ è¿‡å¤§æ–‡ä»¶
        file_size = abs_path.stat().st_size
        if file_size > 10 * 1024 * 1024:  # 10MB limit
            return {"error": f"æ–‡ä»¶è¿‡å¤§ï¼Œè¶…è¿‡10MBé™åˆ¶: {file_path}"}
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        try:
            with open(abs_path, 'r', encoding='utf-8') as f:
                content = f.read()
            is_binary = False
        except (UnicodeDecodeError, Exception):
            # å¦‚æœæ˜¯äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œè¯»å–ä¸ºbase64
            import base64
            with open(abs_path, 'rb') as f:
                binary_content = f.read()
            content = base64.b64encode(binary_content).decode('utf-8')
            is_binary = True
        
        return {
            "success": True,
            "path": str(abs_path),
            "filename": abs_path.name,
            "content": content,
            "size": file_size,
            "is_binary": is_binary
        }
        
    except Exception as e:
        return {"error": f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}"}


def get_shell_info():
    """è·å–å½“å‰shellç¯å¢ƒå’Œç›®å½•ä¿¡æ¯"""
    shell = os.environ.get('SHELL', '/bin/sh')
    current_dir = os.getcwd()
    return {
        "shell": shell,
        "current_directory": current_dir
    }


def create_structured_prompt(user_input: str, shell_info: dict, file_info: dict = None) -> list:
    """
    åˆ›å»ºç»“æ„åŒ–çš„æç¤ºä¿¡æ¯
    è¦æ±‚LLMä»¥ç‰¹å®šæ ¼å¼è¿”å›å¯ç›´æ¥ä½¿ç”¨çš„å‘½ä»¤
    """
    
    # æ„å»ºç³»ç»Ÿæç¤º
    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªå‘½ä»¤è¡ŒåŠ©æ‰‹ï¼Œå¸®åŠ©ç”¨æˆ·è§£å†³shellå‘½ä»¤ç›¸å…³é—®é¢˜ã€‚

å½“å‰ç¯å¢ƒ:
- Shell: {shell_info['shell']}
- å½“å‰ç›®å½•: {shell_info['current_directory']}"""

    # å¦‚æœæœ‰æ–‡ä»¶é™„ä»¶ï¼Œæ·»åŠ æ–‡ä»¶ä¿¡æ¯
    if file_info and file_info.get('success'):
        file_info_text = f"""

æ–‡ä»¶é™„ä»¶ä¿¡æ¯:
- æ–‡ä»¶å: {file_info['filename']}
- æ–‡ä»¶è·¯å¾„: {file_info['path']}
- æ–‡ä»¶å¤§å°: {file_info['size']} bytes
- æ˜¯å¦ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶: {'æ˜¯' if file_info['is_binary'] else 'å¦'}
- æ–‡ä»¶å†…å®¹: 
{file_info['content'] if not file_info['is_binary'] else '[äºŒè¿›åˆ¶å†…å®¹ï¼Œå·²ç¼–ç ä¸ºbase64]'}"""
        
        system_prompt += file_info_text
    
    system_prompt += """

å¦‚æœç”¨æˆ·çš„é—®é¢˜æ˜¯å…³äºå¦‚ä½•è¾“å…¥bash/zshå‘½ä»¤çš„ï¼Œä½ å¿…é¡»ä»¥ä»¥ä¸‹æ ¼å¼è¿”å›å¯ä»¥ç›´æ¥ä½¿ç”¨çš„å‘½ä»¤:
```command
å…·ä½“çš„å‘½ä»¤å†…å®¹
```

å¦‚æœé—®é¢˜ä¸æ¶‰åŠå‘½ä»¤ï¼Œåˆ™æ­£å¸¸å›ç­”å³å¯ã€‚

è¦æ±‚:
1. å¯¹äºéœ€è¦å‘½ä»¤çš„é—®ç­”ï¼Œå¿…é¡»ä½¿ç”¨ä¸Šé¢çš„æ ¼å¼å°†å‘½ä»¤åŒ…è£¹åœ¨```commandä»£ç å—ä¸­ã€‚

ç¤ºä¾‹:
ç”¨æˆ·: "æ€ä¹ˆåˆ—å‡ºå½“å‰ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶,å¹¶ä¸”èƒ½çœ‹åˆ°æ¯ä¸ªæ–‡ä»¶çš„æ‰©å±•åå’Œæ–‡ä»¶å¤§å°?"

ä½ çš„å›ç­”åº”è¯¥æ˜¯:
```command
ls -l
```
"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_input}
    ]

    return messages


def call_llm(messages: list) -> str:
    """è°ƒç”¨OpenAIå…¼å®¹çš„API"""
    try:
        client = OpenAI(
            api_key=os.environ.get('LLM_API_KEY'),
            base_url=os.environ.get('LLM_BASE_URL')
        )

        # æ„å»ºAPIå‚æ•°
        api_params = {
            "model": os.environ.get('LLM_MODEL_NAME', 'doubao-seed-1.6-flash'),
            "messages": messages,
            "max_tokens": 1000,
            "temperature": 0.3
        }

        # å¦‚æœæœ‰æ–‡ä»¶é™„ä»¶ï¼Œç›´æ¥åœ¨ç³»ç»Ÿæç¤ºä¸­åŒ…å«æ–‡ä»¶å†…å®¹ï¼ˆä¸ä½¿ç”¨image_urlæ ¼å¼ï¼‰
        # æˆ‘ä»¬å·²ç»åœ¨create_structured_promptä¸­å¤„ç†äº†æ–‡ä»¶å†…å®¹
        # è¿™é‡Œä¸å†éœ€è¦ç‰¹æ®Šå¤„ç†

        response = client.chat.completions.create(**api_params)

        return response.choices[0].message.content

    except Exception as e:
        return f"Error calling LLM: {str(e)}"


def extract_command(llm_response: str) -> str:
    """
    ä»LLMå“åº”ä¸­æå–å‘½ä»¤
    å¦‚æœæ‰¾åˆ°```commandä»£ç å—ï¼Œè¿”å›å…¶ä¸­çš„å‘½ä»¤å†…å®¹
    """
    import re

    # åŒ¹é…```commandä»£ç å—
    pattern = r'```command\s*\n(.*?)\n```'
    match = re.search(pattern, llm_response, re.DOTALL)
    if match:
        return match.group(1).strip()
    return None


def ensure_llm_env() -> None:
    """æ£€æŸ¥å¿…è¦çš„ç¯å¢ƒå˜é‡æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™æç¤ºåé€€å‡º"""
    required_vars = ['LLM_API_KEY', 'LLM_BASE_URL', 'LLM_MODEL_NAME']
    missing = [v for v in required_vars if not os.environ.get(v)]
    if missing:
        print(f"âŒ ç¼ºå°‘å¿…è¦ç¯å¢ƒå˜é‡: {', '.join(missing)}")
        print("è¯·å…ˆè¿è¡Œ: source llm-switch å¹¶ç¡®ä¿å·²è®¾ç½® LLM_API_KEY, LLM_BASE_URL, LLM_MODEL_NAME")
        sys.exit(2)


def main():
    # ä½¿ç”¨argparseè§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description='LLM Inline - OpenAI-compatible command line LLM interface'
    )
    parser.add_argument('ask', help='Ask a question to LLM')
    parser.add_argument('question', nargs='*', help='Your question to the LLM')
    parser.add_argument('--file', '-f', help='File path to attach to the query')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é—®é¢˜
    if not args.question:
        print("âŒ è¯·æä¾›é—®é¢˜")
        print("Usage: llmi ask \"your question here\" [--file file_path]")
        sys.exit(1)
    
    user_input = " ".join(args.question).strip()
    file_path = args.file
    
    print(f"ğŸ¤” ç”¨æˆ·æé—®: {user_input}")
    if file_path:
        print(f"ğŸ“ é™„ä»¶æ–‡ä»¶: {file_path}")
    print()

    # è·å–shellä¿¡æ¯
    shell_info = get_shell_info()
    
    # å¤„ç†æ–‡ä»¶é™„ä»¶
    file_info = None
    if file_path:
        print("ğŸ“‚ æ­£åœ¨è¯»å–æ–‡ä»¶...")
        file_info = read_file_content(file_path)
        if file_info.get('error'):
            print(f"âŒ {file_info['error']}")
            sys.exit(1)
        print(f"âœ… æ–‡ä»¶è¯»å–æˆåŠŸ: {file_info['filename']} ({file_info['size']} bytes)")
        print()

    # åˆ›å»ºç»“æ„åŒ–æç¤º
    messages = create_structured_prompt(user_input, shell_info, file_info)

    # ç¡®ä¿ç¯å¢ƒ
    ensure_llm_env()

    # è°ƒç”¨LLM
    print("ğŸ§  æ­£åœ¨æ€è€ƒ...")
    llm_response = call_llm(messages)

    if llm_response.startswith("Error"):
        print(f"{llm_response}")
        sys.exit(1)

    # æå–å‘½ä»¤
    command = extract_command(llm_response)

    print("\nğŸ’¡ LLMå›ç­”:")
    print(llm_response)
    print()

    # å¦‚æœæœ‰å‘½ä»¤ï¼Œæç¤ºç”¨æˆ·å¯ä»¥ä½¿ç”¨
    if command:
        print("=" * 50)
        print("ğŸ“‹ å»ºè®®å‘½ä»¤:")
        print(command)
        print("\nğŸ’¡ æç¤º: æ‚¨å¯ä»¥ä½¿ç”¨Tabé”®å¿«é€Ÿç²˜è´´æ­¤å‘½ä»¤")

        # å°†å‘½ä»¤ç¼“å­˜åˆ°æ–‡ä»¶ï¼Œä¾› shell æŒ‰é”®ç»‘å®šè¯»å–
        try:
            cache_dir = Path(os.path.expanduser("~/.cache/llmi"))
            cache_dir.mkdir(parents=True, exist_ok=True)
            (cache_dir / "last_command").write_text(command + "\n", encoding="utf-8")
        except Exception as _:
            pass

    return command


if __name__ == "__main__":
    main()