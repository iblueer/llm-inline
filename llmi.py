#!/usr/bin/env python3
"""
LLM Inline - OpenAI-compatible command line LLM interface

Usage: llmi ask "your question here"
"""

import os
import sys
import json
import subprocess
from openai import OpenAI
from pathlib import Path


def get_shell_info():
    """è·å–å½“å‰shellç¯å¢ƒå’Œç›®å½•ä¿¡æ¯"""
    shell = os.environ.get('SHELL', '/bin/sh')
    current_dir = os.getcwd()
    return {
        "shell": shell,
        "current_directory": current_dir
    }


def create_structured_prompt(user_input: str, shell_info: dict) -> list:
    """
    åˆ›å»ºç»“æ„åŒ–çš„æç¤ºä¿¡æ¯
    è¦æ±‚LLMä»¥ç‰¹å®šæ ¼å¼è¿”å›å¯ç›´æ¥ä½¿ç”¨çš„å‘½ä»¤
    """
    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªå‘½ä»¤è¡ŒåŠ©æ‰‹ï¼Œå¸®åŠ©ç”¨æˆ·è§£å†³shellå‘½ä»¤ç›¸å…³é—®é¢˜ã€‚

å½“å‰ç¯å¢ƒ:
- Shell: {shell_info['shell']}
- å½“å‰ç›®å½•: {shell_info['current_directory']}

å¦‚æœç”¨æˆ·çš„é—®é¢˜æ˜¯å…³äºå¦‚ä½•è¾“å…¥bash/zshå‘½ä»¤çš„ï¼Œä½ å¿…é¡»ä»¥ä»¥ä¸‹æ ¼å¼è¿”å›å¯ä»¥ç›´æ¥ä½¿ç”¨çš„å‘½ä»¤:
```command
å…·ä½“çš„å‘½ä»¤å†…å®¹
```

å¦‚æœé—®é¢˜ä¸æ¶‰åŠå‘½ä»¤ï¼Œåˆ™æ­£å¸¸å›ç­”å³å¯ã€‚

è¦æ±‚:
1. å¯¹äºéœ€è¦å‘½ä»¤çš„é—®ç­”ï¼Œå¿…é¡»ä½¿ç”¨ä¸Šé¢çš„æ ¼å¼å°†å‘½ä»¤åŒ…è£¹åœ¨```commandä»£ç å—ä¸­ã€‚

ç¤ºä¾‹:
ç”¨æˆ·: "æ€ä¹ˆåˆ—å‡ºå½“å‰ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶,å¹¶ä¸”èƒ½çœ‹åˆ°æ¯ä¸ªæ–‡ä»¶çš„æ‰©å±•åå’Œæ–‡ä»¶å¤§å°?"

ä½ çš„å›ç­”åº”è¯¥æ˜¯:
```command
ls -l
```
"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_input}
    ]

    return messages


def call_llm(messages: list) -> str:
    """è°ƒç”¨OpenAIå…¼å®¹çš„API"""
    try:
        client = OpenAI(
            api_key=os.environ.get('LLM_API_KEY'),
            base_url=os.environ.get('LLM_BASE_URL')
        )

        response = client.chat.completions.create(
            model=os.environ.get('LLM_MODEL_NAME', 'doubao-seed-1.6-flash'),
            messages=messages,
            max_tokens=1000,
            temperature=0.3
        )

        return response.choices[0].message.content

    except Exception as e:
        return f"Error calling LLM: {str(e)}"


def extract_command(llm_response: str) -> str:
    """
    ä»LLMå“åº”ä¸­æå–å‘½ä»¤
    å¦‚æœæ‰¾åˆ°```commandä»£ç å—ï¼Œè¿”å›å…¶ä¸­çš„å‘½ä»¤å†…å®¹
    """
    import re

    # åŒ¹é…```commandä»£ç å—
    pattern = r'```command\s*\n(.*?)\n```'
    match = re.search(pattern, llm_response, re.DOTALL)
    if match:
        return match.group(1).strip()
    return None


def ensure_llm_env() -> None:
    """æ£€æŸ¥å¿…è¦çš„ç¯å¢ƒå˜é‡æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™æç¤ºåé€€å‡º"""
    required_vars = ['LLM_API_KEY', 'LLM_BASE_URL', 'LLM_MODEL_NAME']
    missing = [v for v in required_vars if not os.environ.get(v)]
    if missing:
        print(f"âŒ ç¼ºå°‘å¿…è¦ç¯å¢ƒå˜é‡: {', '.join(missing)}")
        print("è¯·å…ˆè¿è¡Œ: source llm-switch å¹¶ç¡®ä¿å·²è®¾ç½® LLM_API_KEY, LLM_BASE_URL, LLM_MODEL_NAME")
        sys.exit(2)


def main():
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) < 3 or sys.argv[1] != "ask":
        print("Usage: llmi ask \"your question here\"")
        sys.exit(1)

    user_input = " ".join(sys.argv[2:]).strip()

    print(f"ğŸ¤” ç”¨æˆ·æé—®: {user_input}")
    print()

    # è·å–shellä¿¡æ¯
    shell_info = get_shell_info()

    # åˆ›å»ºç»“æ„åŒ–æç¤º
    messages = create_structured_prompt(user_input, shell_info)

    # ç¡®ä¿ç¯å¢ƒ
    ensure_llm_env()

    # è°ƒç”¨LLM
    print("ğŸ§  æ­£åœ¨æ€è€ƒ...")
    llm_response = call_llm(messages)

    if llm_response.startswith("Error"):
        print(f"{llm_response}")
        sys.exit(1)

    # æå–å‘½ä»¤
    command = extract_command(llm_response)

    print("\nğŸ’¡ LLMå›ç­”:")
    print(llm_response)
    print()

    # å¦‚æœæœ‰å‘½ä»¤ï¼Œæç¤ºç”¨æˆ·å¯ä»¥ä½¿ç”¨
    if command:
        print("=" * 50)
        print("ğŸ“‹ å»ºè®®å‘½ä»¤:")
        print(command)
        print("\nğŸ’¡ æç¤º: æ‚¨å¯ä»¥ä½¿ç”¨Tabé”®å¿«é€Ÿç²˜è´´æ­¤å‘½ä»¤")

        # å°†å‘½ä»¤ç¼“å­˜åˆ°æ–‡ä»¶ï¼Œä¾› shell æŒ‰é”®ç»‘å®šè¯»å–
        try:
            cache_dir = Path(os.path.expanduser("~/.cache/llmi"))
            cache_dir.mkdir(parents=True, exist_ok=True)
            (cache_dir / "last_command").write_text(command + "\n", encoding="utf-8")
        except Exception as _:
            pass

    return command


if __name__ == "__main__":
    main()